{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a4abcd-b2ed-49ed-bb6a-db960a47d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import soundfile\n",
    "import librosa\n",
    "import torchlibrosa\n",
    "from panns_inference import AudioTagging\n",
    "#STFT - Short-time Fourier transform\n",
    "\n",
    "#The short-time Fourier transform (STFT) is a Fourier-related transform used to determine the sinusoidal \n",
    "#frequency and phase content of local sections of a signal as it changes over time.[1] In practice, the \n",
    "#procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and \n",
    "#then compute the Fourier transform separately on each shorter segment. This reveals the Fourier spectrum \n",
    "#on each shorter segment. One then usually plots the changing spectra as a function of time, \n",
    "#known as a spectrogram or waterfall plot, such as commonly used in software defined radio (SDR) based \n",
    "#spectrum displays. Full bandwidth displays covering the whole range of an SDR commonly use fast Fourier transforms (FFTs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e963929-aea0-4257-a3e4-82dc00a0d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def move_data_to_device(x, device):\n",
    "    if 'float' in str(x.dtype):\n",
    "        x = torch.Tensor(x)\n",
    "    elif 'int' in str(x.dtype):\n",
    "        x = torch.LongTensor(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    return x.to(device)\n",
    "\n",
    "\n",
    "def do_mixup(x, mixup_lambda):\n",
    "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n",
    "    (1, 3, 5, ...).\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size * 2, ...)\n",
    "      mixup_lambda: (batch_size * 2,)\n",
    "\n",
    "    Returns:\n",
    "      out: (batch_size, ...)\n",
    "    \"\"\"\n",
    "    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n",
    "        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n",
    "    return out\n",
    "    \n",
    "\n",
    "def append_to_dict(dict, key, value):\n",
    "    if key in dict.keys():\n",
    "        dict[key].append(value)\n",
    "    else:\n",
    "        dict[key] = [value]\n",
    "\n",
    "\n",
    "def forward(model, generator, return_input=False, \n",
    "    return_target=False):\n",
    "    \"\"\"Forward data to a model.\n",
    "    \n",
    "    Args: \n",
    "      model: object\n",
    "      generator: object\n",
    "      return_input: bool\n",
    "      return_target: bool\n",
    "\n",
    "    Returns:\n",
    "      audio_name: (audios_num,)\n",
    "      clipwise_output: (audios_num, classes_num)\n",
    "      (ifexist) segmentwise_output: (audios_num, segments_num, classes_num)\n",
    "      (ifexist) framewise_output: (audios_num, frames_num, classes_num)\n",
    "      (optional) return_input: (audios_num, segment_samples)\n",
    "      (optional) return_target: (audios_num, classes_num)\n",
    "    \"\"\"\n",
    "    output_dict = {}\n",
    "    device = next(model.parameters()).device\n",
    "    time1 = time.time()\n",
    "\n",
    "    # Forward data to a model in mini-batches\n",
    "    for n, batch_data_dict in enumerate(generator):\n",
    "        print(n)\n",
    "        batch_waveform = move_data_to_device(batch_data_dict['waveform'], device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            batch_output = model(batch_waveform)\n",
    "\n",
    "        append_to_dict(output_dict, 'audio_name', batch_data_dict['audio_name'])\n",
    "\n",
    "        append_to_dict(output_dict, 'clipwise_output', \n",
    "            batch_output['clipwise_output'].data.cpu().numpy())\n",
    "\n",
    "        if 'segmentwise_output' in batch_output.keys():\n",
    "            append_to_dict(output_dict, 'segmentwise_output', \n",
    "                batch_output['segmentwise_output'].data.cpu().numpy())\n",
    "\n",
    "        if 'framewise_output' in batch_output.keys():\n",
    "            append_to_dict(output_dict, 'framewise_output', \n",
    "                batch_output['framewise_output'].data.cpu().numpy())\n",
    "            \n",
    "        if return_input:\n",
    "            append_to_dict(output_dict, 'waveform', batch_data_dict['waveform'])\n",
    "            \n",
    "        if return_target:\n",
    "            if 'target' in batch_data_dict.keys():\n",
    "                append_to_dict(output_dict, 'target', batch_data_dict['target'])\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            print(' --- Inference time: {:.3f} s / 10 iterations ---'.format(\n",
    "                time.time() - time1))\n",
    "            time1 = time.time()\n",
    "\n",
    "    for key in output_dict.keys():\n",
    "        output_dict[key] = np.concatenate(output_dict[key], axis=0)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the \n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    \n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value \n",
    "    is the same as the value of the last frame.\n",
    "\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1 :, :].repeat(1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_flops(model, audio_length):\n",
    "    \"\"\"Count flops. Code modified from others' implementation.\n",
    "    \"\"\"\n",
    "    multiply_adds = True\n",
    "    list_conv2d=[]\n",
    "    def conv2d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    " \n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_height * output_width\n",
    " \n",
    "        list_conv2d.append(flops)\n",
    "\n",
    "    list_conv1d=[]\n",
    "    def conv1d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_length = input[0].size()\n",
    "        output_channels, output_length = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size[0] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    " \n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_length\n",
    " \n",
    "        list_conv1d.append(flops)\n",
    " \n",
    "    list_linear=[] \n",
    "    def linear_hook(self, input, output):\n",
    "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
    " \n",
    "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
    "        bias_ops = self.bias.nelement()\n",
    " \n",
    "        flops = batch_size * (weight_ops + bias_ops)\n",
    "        list_linear.append(flops)\n",
    " \n",
    "    list_bn=[] \n",
    "    def bn_hook(self, input, output):\n",
    "        list_bn.append(input[0].nelement() * 2)\n",
    " \n",
    "    list_relu=[] \n",
    "    def relu_hook(self, input, output):\n",
    "        list_relu.append(input[0].nelement() * 2)\n",
    " \n",
    "    list_pooling2d=[]\n",
    "    def pooling2d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size * self.kernel_size\n",
    "        bias_ops = 0\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_height * output_width\n",
    " \n",
    "        list_pooling2d.append(flops)\n",
    "\n",
    "    list_pooling1d=[]\n",
    "    def pooling1d_hook(self, input, output):\n",
    "        batch_size, input_channels, input_length = input[0].size()\n",
    "        output_channels, output_length = output[0].size()\n",
    " \n",
    "        kernel_ops = self.kernel_size[0]\n",
    "        bias_ops = 0\n",
    "        \n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_length\n",
    " \n",
    "        list_pooling2d.append(flops)\n",
    " \n",
    "    def foo(net):\n",
    "        childrens = list(net.children())\n",
    "        if not childrens:\n",
    "            if isinstance(net, nn.Conv2d):\n",
    "                net.register_forward_hook(conv2d_hook)\n",
    "            elif isinstance(net, nn.Conv1d):\n",
    "                net.register_forward_hook(conv1d_hook)\n",
    "            elif isinstance(net, nn.Linear):\n",
    "                net.register_forward_hook(linear_hook)\n",
    "            elif isinstance(net, nn.BatchNorm2d) or isinstance(net, nn.BatchNorm1d):\n",
    "                net.register_forward_hook(bn_hook)\n",
    "            elif isinstance(net, nn.ReLU):\n",
    "                net.register_forward_hook(relu_hook)\n",
    "            elif isinstance(net, nn.AvgPool2d) or isinstance(net, nn.MaxPool2d):\n",
    "                net.register_forward_hook(pooling2d_hook)\n",
    "            elif isinstance(net, nn.AvgPool1d) or isinstance(net, nn.MaxPool1d):\n",
    "                net.register_forward_hook(pooling1d_hook)\n",
    "            else:\n",
    "                print('Warning: flop of module {} is not counted!'.format(net))\n",
    "            return\n",
    "        for c in childrens:\n",
    "            foo(c)\n",
    "\n",
    "    # Register hook\n",
    "    foo(model)\n",
    "    \n",
    "    device = device = next(model.parameters()).device\n",
    "    input = torch.rand(1, audio_length).to(device)\n",
    "\n",
    "    out = model(input)\n",
    " \n",
    "    total_flops = sum(list_conv2d) + sum(list_conv1d) + sum(list_linear) + \\\n",
    "        sum(list_bn) + sum(list_relu) + sum(list_pooling2d) + sum(list_pooling1d)\n",
    "    \n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e252d207-c2df-4e45-b8f7-18386cb164c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"window_size\": 512, \n",
    "    \"hop_size\": 160,\n",
    "    \"mel_bins\": 64, \n",
    "    \"fmin\": 50, \n",
    "    \"fmax\": 8000, \n",
    "    \"classes_num\": 527\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "244f1a29-d892-46db-8f2e-2f3e8395b1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a89b3ec-0b66-437c-b55f-46fe689fa044",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"artifacts/Cnn14_mAP=0.431.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "168e3dd5-5c56-46fd-8189-729cc3ce608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: C:\\Users\\RomanKyrkalo/panns_data/Cnn14_mAP=0.431.pth\n",
      "GPU number: 1\n",
      "Bird vocalization, bird call, bird song :  0.5647661089897156\n",
      "Environmental noise :  0.41335853934288025\n",
      "Bird :  0.38755905628204346\n",
      "Chirp, tweet :  0.3797639310359955\n",
      "Outside, rural or natural :  0.0777573436498642\n",
      "Music :  0.06243884563446045\n",
      "Animal :  0.030938906595110893\n",
      "Speech :  0.027243217453360558\n",
      "Owl :  0.020749123767018318\n",
      "Insect :  0.016712162643671036\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from panns_inference import AudioTagging, labels\n",
    "\n",
    "audio_path = r\"C:/Users/RomanKyrkalo/Downloads/download.wav\"\n",
    "\n",
    "# 1) Load mono audio at 32 kHz\n",
    "y, sr = librosa.load(audio_path, sr=32000, mono=True)\n",
    "\n",
    "# 2) Add batch dimension: (1, T)\n",
    "y = y[None, :]\n",
    "\n",
    "# 3) Run inference\n",
    "model = AudioTagging(checkpoint_path=None)  # will download CNN14 weights on first use\n",
    "clipwise_scores, embedding = model.inference(y)  # clipwise_scores: (1, 527)\n",
    "\n",
    "# 4) Show top-10 labels\n",
    "topk = np.argsort(-clipwise_scores[0])[:10]\n",
    "for k in topk:\n",
    "    print(labels[k], \": \", float(clipwise_scores[0, k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75c1975b-9b8c-4aba-81ea-dad3252a75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<panns_inference.inference.AudioTagging object at 0x00000111BFBA4B90>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ddda8-af8d-4a44-968e-3670abae4fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
